<!DOCTYPE html>
<html lang="en" class="dark">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>üìê Analysis III | ISI B.Stat Sem-3</title>

  <!-- Tailwind CSS -->
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      darkMode: 'class',
      theme: {
        extend: {
          fontFamily: {
            sans: ['Inter', 'sans-serif'],
          },
          colors: {
            primary: '#3b82f6',
          },
        },
      },
    };
  </script>

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap" rel="stylesheet" />
  <link rel="icon" href="/assets/favicon.png" />

  <!-- MathJax -->
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <!-- Custom Styles -->
  <style>
    html {
      scroll-behavior: smooth;
    }

    body {
      background: linear-gradient(135deg, #000000, #1e293b, #111827);
      background-size: 400% 400%;
      animation: gradientShift 15s ease infinite;
    }

    @keyframes gradientShift {
      0% { background-position: 0% 50%; }
      50% { background-position: 100% 50%; }
      100% { background-position: 0% 50%; }
    }

    details[open] summary::after {
      transform: rotate(90deg);
    }
  </style>
</head>

<body class="text-white font-sans min-h-screen">

  <!-- Header -->
  <header class="sticky top-0 z-50 bg-black/30 backdrop-blur border-b border-gray-700 shadow-md px-6 py-4 flex justify-between items-center">
    <h1 class="text-2xl font-bold tracking-wide">üìê Analysis III</h1>
    <a href="https://statshaan27.github.io/Shaan/" class="text-sm text-gray-400 hover:text-white">‚Üê Back to Index</a>
  </header>

  <!-- Layout -->
  <div class="flex">
    
    <!-- Collapsible Ribbon Sidebar -->
    <aside class="w-64 p-4 border-r border-gray-800 bg-gray-900/40 backdrop-blur h-screen overflow-y-auto hidden md:block">
      <nav class="space-y-4 text-md">
        <details class="group" open>
          <summary class="cursor-pointer font-semibold text-primary">Week 1</summary>
          <ul class="ml-4 mt-1 space-y-1 text-gray-300">
            <li><a href="#sec1" class="hover:text-white">Limits</a></li>
            <li><a href="#sec2" class="hover:text-white">Continuity</a></li>
          </ul>
        </details>
        <details class="group">
          <summary class="cursor-pointer font-semibold text-primary">Week 2</summary>
          <ul class="ml-4 mt-1 space-y-1 text-gray-300">
            <li><a href="#sec3" class="hover:text-white">Differentiability</a></li>
            <li><a href="#sec4" class="hover:text-white">Mean Value Theorem</a></li>
          </ul>
        </details>
        <!-- Add more weeks/sections as needed -->
      </nav>
    </aside>

    <!-- Main Content Area -->
    <main class="flex-1 p-6 overflow-x-auto">
      <section class="bg-white/5 backdrop-blur-sm p-6 rounded-2xl shadow-lg border border-gray-700">
        <!-- üß† Content Goes Here -->
        <!-- Just paste <section> blocks here for each note -->
        <h2 class="text-xl font-semibold mb-4 text-primary">Welcome to Analysis III</h2>
        <p class="text-gray-300">Use the collapsible sidebar on the left to jump between topics.</p>
      <section>

  <h4>üß† Variance Inequality and the Cram√©r‚ÄìRao Bound</h4>

  <p>You‚Äôre discussing a <strong>variance inequality</strong> in the context of estimation theory, specifically a <strong>Cram√©r‚ÄìRao-type bound</strong>.</p>

  <h4>1. Setup:</h4>

  <p>Let \( X_1, X_2, \dots, X_n \) be i.i.d. with density \( f(x|\theta) \), and let \( W(\mathbf{X}) \) be a statistic where \( \mathbf{X} = (X_1, \dots, X_n) \).</p>

  <p>The goal is to study the variance of this statistic and establish a lower bound. The assumptions are:</p>

  <ul>
    <li>\( \operatorname{Var}_\theta(W(\mathbf{X})) < \infty \)</li>
    <li>\( \frac{d}{d\theta} \mathbb{E}_\theta[W(\mathbf{X})] = \int \frac{d}{d\theta}\left(W(\mathbf{X}) f(\mathbf{X}|\theta)\right) d\mathbf{x} \)</li>
  </ul>

  <p>This second condition is a <em>technical regularity condition</em> that allows the interchange of differentiation and integration ‚Äî a standard step in proving the Cram√©r‚ÄìRao inequality.</p>

  <h4>2. The Inequality Itself:</h4>

  <p>The general form of the Cram√©r‚ÄìRao bound is:</p>

  \[
  \operatorname{Var}_\theta(W(\mathbf{X})) \geq \frac{\left( \frac{d}{d\theta} \mathbb{E}_\theta[W(\mathbf{X})] \right)^2}{\mathbb{E}_\theta\left[ \left( \frac{d}{d\theta} \log f(\mathbf{X}|\theta) \right)^2 \right]}
  \]

  <p>This inequality states that the variance of any (possibly biased) statistic is lower-bounded by a ratio involving its sensitivity to the parameter and the Fisher Information.</p>

  <p>The denominator is the <strong>Fisher Information</strong>, representing how much information the data \( \mathbf{X} \) provides about the parameter \( \theta \).</p>

  <h4>3. Simplification (1): Unbiased Estimator</h4>

  <p>If the statistic is <strong>unbiased</strong>, i.e.,</p>

  \[
  \mathbb{E}_\theta[W(\mathbf{X})] = \theta \quad \text{for all } \theta,
  \]

  <p>then:</p>

  \[
  \frac{d}{d\theta} \mathbb{E}_\theta[W(\mathbf{X})] = 1
  \]

  <p>Substituting this into the inequality, the bound simplifies to:</p>

  \[
  \operatorname{Var}_\theta(W(\mathbf{X})) \geq \frac{1}{\mathbb{E}_\theta\left[ \left( \frac{d}{d\theta} \log f(\mathbf{X}|\theta) \right)^2 \right]}
  \]

  <p>This is the <strong>standard Cram√©r‚ÄìRao Lower Bound (CRLB)</strong> for unbiased estimators.</p>

  <h4>4. Example:</h4>

  <p>If \( \mathbb{E}(X_i) = \theta \) for all \( i \), and \( W(\mathbf{X}) \) is the sample mean, then clearly:</p>

  \[
  \mathbb{E}(W(\mathbf{X})) = \theta
  \]

  <p>‚úÖ Thus, the sample mean is an unbiased estimator of \( \theta \) and qualifies under Simplification (1).</p>

  <h4>‚úÖ Summary So Far:</h4>

  <ul>
    <li>You‚Äôve established a general variance lower bound involving Fisher Information.</li>
    <li>You‚Äôve stated the regularity condition needed for interchanging differentiation and integration.</li>
    <li>You‚Äôve shown how the inequality simplifies when the estimator is unbiased.</li>
    <li>You‚Äôve provided a concrete example where this applies ‚Äî the sample mean.</li>
  </ul>

</section>
<section>

  <h4>üîç Simplification (2): Cram√©r‚ÄìRao Bound for IID Case</h4>

  <h4>1. Factorization of Likelihood for IID Samples</h4>

  <p>If \( X_1, \dots, X_n \) are <strong>i.i.d.</strong>, then the joint density factorizes as:</p>

  \[
  f(\mathbf{x}|\theta) = \prod_{i=1}^{n} f(x_i|\theta)
  \quad \Rightarrow \quad
  \log f(\mathbf{x}|\theta) = \sum_{i=1}^{n} \log f(x_i|\theta)
  \]

  <p>This simplification makes it easier to compute the <em>score function</em>, which is the derivative of the log-likelihood with respect to the parameter.</p>

  <h4>2. Example: \( X_i \sim \mathcal{N}(\mu, 1) \)</h4>

  <p>Given that:</p>

  \[
  f(x_i|\mu) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(x_i - \mu)^2}{2}\right)
  \]

  <p>Taking the logarithm of the density:</p>

  \[
  \log f(x_i|\mu) = -\frac{1}{2} \log(2\pi) - \frac{1}{2}(x_i - \mu)^2
  \]

  <p>Differentiating with respect to \( \mu \):</p>

  \[
  \frac{d}{d\mu} \log f(x_i|\mu) = x_i - \mu
  \]

  <p>So the <strong>score function</strong> for the full sample becomes:</p>

  \[
  \frac{d}{d\mu} \log f(\mathbf{x}|\mu) = \sum_{i=1}^n (x_i - \mu)
  \]

  <h4>3. Fisher Information</h4>

  <p>To compute the Fisher Information \( \mathcal{I}(\mu) \), consider:</p>

  \[
  \mathbb{E}_\mu \left[ \left( \sum_{i=1}^n (X_i - \mu) \right)^2 \right]
  = \sum_{i=1}^n \mathbb{E}_\mu[(X_i - \mu)^2]
  \]

  <p>The cross terms vanish because the variables are independent and each \( \mathbb{E}_\mu[X_i - \mu] = 0 \). Each term contributes 1, since:</p>

  \[
  \mathbb{E}_\mu[(X_i - \mu)^2] = \operatorname{Var}(X_i) = 1
  \]

  <p>So the total Fisher Information is:</p>

  \[
  \mathcal{I}(\mu) = n
  \]

  <h4>‚úÖ Conclusion:</h4>

  <p>You‚Äôve derived the CRLB for the i.i.d. normal case \( \mathcal{N}(\mu, 1) \). Key results:</p>

  <ul>
    <li><strong>Estimator:</strong> Sample mean \( \bar{X} \)</li>
    <li><strong>Unbiased:</strong> Yes</li>
    <li><strong>Variance of \( \bar{X} \):</strong> \( \frac{1}{n} \)</li>
    <li><strong>Fisher Information:</strong> \( n \)</li>
    <li><strong>CRLB:</strong> \( \frac{1}{n} \), which is achieved</li>
  </ul>

  <p>‚úÖ Hence, the sample mean is an <strong>efficient estimator</strong> of \( \mu \) in this setting.</p>

</section>
<section>

  <h4>‚úÖ Putting It All Together</h4>

  <p>You‚Äôve now made two key simplifications to understand how the Cram√©r‚ÄìRao bound plays out in practice:</p>

  <h4>Simplification (1):</h4>

  <p>If \( \mathbb{E}_\mu[W(\mathbf{X})] = \mu \), then:</p>

  <ul>
    <li>\( W(\mathbf{X}) \) is an <strong>unbiased</strong> estimator of \( \mu \)</li>
    <li>The numerator in CRLB becomes 1</li>
  </ul>

  \[
  \operatorname{Var}_\mu(W(\mathbf{X})) \geq \frac{1}{\mathbb{E}_\mu\left[ \left( \frac{d}{d\mu} \log f(\mathbf{X}|\mu) \right)^2 \right]} = \frac{1}{\text{Fisher Information}}
  \]

  <h4>Simplification (2):</h4>

  <ul>
    <li>For the i.i.d. normal case, Fisher Information = \( n \)</li>
    <li>So, CRLB = \( \frac{1}{n} \)</li>
  </ul>

  <h4>‚ùì Immediate Question: Does the sample mean attain this lower bound?</h4>

  <p>The sample mean is defined as:</p>

  \[
  \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
  \]

  <p>Given \( X_i \sim \mathcal{N}(\mu, 1) \), we have:</p>

  \[
  \mathbb{E}[\bar{X}] = \mu, \quad \operatorname{Var}(\bar{X}) = \frac{1}{n}
  \]

  <p>‚úÖ Therefore:</p>

  <ul>
    <li>The sample mean is <strong>unbiased</strong>: satisfies Simplification (1)</li>
    <li>Its variance \( \frac{1}{n} \) matches the CRLB from Simplification (2)</li>
  </ul>

  <h4>‚úÖ Final Answer:</h4>

  <p><strong>Yes</strong>, the sample mean <strong>attains the Cram√©r‚ÄìRao lower bound</strong> for the normal model with known variance. It is an <strong>efficient estimator</strong> of \( \mu \).</p>

  <hr>

  <h4>üîÅ Cram√©r‚ÄìRao Inequality in the Discrete Case</h4>

  <p>Let‚Äôs transition to the discrete version of the CRLB, where the underlying distribution is a <strong>PMF</strong> instead of a PDF.</p>

  <h4>Continuous Case Recap:</h4>

  <ul>
    <li>We assume a differentiable <strong>pdf</strong> \( f(x|\theta) \)</li>
    <li>We require regularity conditions to differentiate under the integral</li>
    <li>Fisher Information is defined using \( \frac{d}{d\theta} \log f(x|\theta) \)</li>
  </ul>

  <h4>üì¶ Discrete Case Setup:</h4>

  <p>We now work with a <strong>pmf</strong> \( p(x|\theta) \), where \( x \in \mathcal{X} \), a countable set.</p>

  <h4>üß† Key Result:</h4>

  <p>Suppose:</p>

  <ul>
    <li>\( X_1, \dots, X_n \) are i.i.d. with pmf \( p(x|\theta) \)</li>
    <li>\( W(\mathbf{X}) \) is a statistic such that:
      <ul>
        <li>\( \mathbb{E}_\theta[W(\mathbf{X})] \) is differentiable</li>
        <li>Derivative of expectation can be interchanged with the sum:
        \[
        \frac{d}{d\theta} \mathbb{E}_\theta[W(\mathbf{X})] = \sum_{\mathbf{x}} W(\mathbf{x}) \frac{d}{d\theta} p(\mathbf{x}|\theta)
        \]
        </li>
      </ul>
    </li>
  </ul>

  <p>Then the <strong>Cram√©r‚ÄìRao inequality</strong> in this setting becomes:</p>

  \[
  \operatorname{Var}_\theta(W(\mathbf{X})) \geq \frac{\left( \frac{d}{d\theta} \mathbb{E}_\theta[W(\mathbf{X})] \right)^2}{\mathbb{E}_\theta\left[ \left( \frac{d}{d\theta} \log p(\mathbf{X}|\theta) \right)^2 \right]}
  \]

  <p>This is structurally identical to the continuous version, with integrals replaced by sums.</p>

  <h4>‚úÖ Example: Bernoulli Case</h4>

  <p>Suppose:</p>

  <ul>
    <li>\( X_1, \dots, X_n \sim \text{Bernoulli}(p) \)</li>
    <li>\( p \in (0, 1) \), and we wish to estimate \( p \)</li>
  </ul>

  <p>The PMF is:</p>

  \[
  p(x_i | p) = p^{x_i}(1-p)^{1 - x_i}
  \]

  <p>Log-likelihood of the sample:</p>

  \[
  \log p(\mathbf{X}|p) = \sum_{i=1}^n \left[ x_i \log p + (1 - x_i) \log(1 - p) \right]
  \]

  <p>Score function:</p>

  \[
  \frac{d}{dp} \log p(\mathbf{X}|p) = \sum_{i=1}^n \left( \frac{x_i}{p} - \frac{1 - x_i}{1 - p} \right)
  \]

  <p>Fisher Information becomes:</p>

  \[
  \mathcal{I}(p) = \mathbb{E}_p\left[ \left( \sum_{i=1}^n \left( \frac{X_i}{p} - \frac{1 - X_i}{1 - p} \right) \right)^2 \right] = \frac{n}{p(1 - p)}
  \]

  <p>The sample mean:</p>

  \[
  \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
  \]

  <p>is unbiased, with variance:</p>

  \[
  \operatorname{Var}(\bar{X}) = \frac{p(1 - p)}{n}
  \]

  <p>So the CRLB is:</p>

  \[
  \operatorname{Var}(\hat{p}) \geq \frac{1}{\mathcal{I}(p)} = \frac{p(1 - p)}{n}
  \]

  <p>‚úÖ This bound is achieved by the sample mean, so it is <strong>efficient</strong>.</p>

  <h4>Summary:</h4>

  <ul>
    <li>The CRLB extends naturally to <strong>discrete distributions</strong>.</li>
    <li>Integrals become sums.</li>
    <li>Work with \( \log p(x|\theta) \) instead of \( \log f(x|\theta) \).</li>
    <li>In the Bernoulli case, the sample mean is an efficient estimator of \( p \).</li>
  </ul>

</section>
<section>

  <h4>üìò Statement of the Result:</h4>

  <p>If the regularity condition holds:</p>

  \[
  \frac{d}{d\theta}\mathbb{E}_{\theta}\left(\frac{\partial}{\partial\theta}\log f(X|\theta)\right) = \int \left[\frac{\partial}{\partial\theta}\left[\left(\frac{\partial}{\partial\theta}\log f(x|\theta)\right) f(x|\theta)\right]\right] dx,
  \]

  <p>then we can derive the identity:</p>

  \[
  \mathbb{E}_\theta\left[\left(\frac{\partial}{\partial\theta} \log f(X|\theta)\right)^2\right] = -\mathbb{E}_\theta\left[\frac{\partial^2}{\partial\theta^2} \log f(X|\theta)\right]
  \]

  <p>This is known as the <strong>Fisher Information identity</strong>. Both sides equal the <strong>Fisher Information</strong> \( \mathcal{I}(\theta) \).</p>

  <hr>

  <h4>üìå Why This Is True: Key Insight</h4>

  <p>Let \( \ell(\theta; X) = \log f(X|\theta) \), the log-likelihood.</p>

  <ul>
    <li>The <strong>score</strong> is \( \frac{\partial}{\partial\theta} \ell(\theta; X) \)</li>
    <li>Its <strong>expectation</strong> is 0:
      \[
      \mathbb{E}_\theta\left[\frac{\partial}{\partial\theta} \ell(\theta; X)\right] = 0
      \]
    </li>
  </ul>

  <p>Differentiating under the integral sign:</p>

  \[
  \frac{d}{d\theta} \mathbb{E}_\theta\left[\frac{\partial}{\partial\theta} \ell(\theta; X)\right] = 0 = \mathbb{E}_\theta\left[ \frac{\partial^2}{\partial\theta^2} \ell(\theta; X) + \left( \frac{\partial}{\partial\theta} \ell(\theta; X) \right)^2 \right]
  \]

  <p>Rearranging gives:</p>

  \[
  \mathbb{E}_\theta\left[\left(\frac{\partial}{\partial\theta} \log f(X|\theta)\right)^2\right] = -\mathbb{E}_\theta\left[\frac{\partial^2}{\partial\theta^2} \log f(X|\theta)\right]
  \]

  <hr>

  <h4>üîÅ Interpretation:</h4>

  <ul>
    <li><strong>LHS</strong> = variance of the score (square of derivative of log-likelihood)</li>
    <li><strong>RHS</strong> = negative expectation of the second derivative (curvature of log-likelihood)</li>
  </ul>

  <p>Both sides quantify <strong>how much information</strong> the data carries about \( \theta \).</p>

  <hr>

  <h4>‚úÖ Summary:</h4>

  <p>You are stating the <strong>Fisher Information identity</strong>, which tells us:</p>

  <blockquote>
    Under regularity conditions, the variance of the score function equals the negative expected second derivative of the log-likelihood.
  </blockquote>

  \[
  \mathcal{I}(\theta) = \operatorname{Var}_\theta\left( \frac{d}{d\theta} \log f(X|\theta) \right) = -\mathbb{E}_\theta\left[ \frac{d^2}{d\theta^2} \log f(X|\theta) \right]
  \]

  <p>This identity is <strong>fundamental</strong> in proving the Cram√©r‚ÄìRao Lower Bound and in the <strong>asymptotic theory</strong> of maximum likelihood estimators.</p>

</section>
<section>
  <h4>Fisher Information Identity</h4>
  <p>
    If the regularity condition holds:
  </p>
  <p>
    $$\frac{d}{d\theta}\mathbb{E}_{\theta}\left(\frac{\partial}{\partial\theta}\log f(X|\theta)\right) = \int \left[\frac{\partial}{\partial\theta}\left[\left(\frac{\partial}{\partial\theta}\log f(x|\theta)\right) f(x|\theta)\right]\right] dx,$$
  </p>
  <p>
    then we can derive:
  </p>
  <p>
    $$\mathbb{E}_\theta\left[\left(\frac{\partial}{\partial\theta} \log f(X|\theta)\right)^2\right] = -\mathbb{E}_\theta\left[\frac{\partial^2}{\partial\theta^2} \log f(X|\theta)\right]$$
  </p>
  <p>
    This is known as the <strong>Fisher Information equality</strong>, and both sides equal the <strong>Fisher Information</strong> \(\mathcal{I}(\theta)\).
  </p>

  <h4>Key Insight</h4>
  <p>
    Let \(\ell(\theta; X) = \log f(X|\theta)\). Then:
    <ul>
      <li>The score is \(\frac{\partial}{\partial\theta} \ell(\theta; X)\)</li>
      <li>Its expectation is 0 under regularity: \(\mathbb{E}_\theta\left[\frac{\partial}{\partial\theta} \ell(\theta; X)\right] = 0\)</li>
    </ul>
    Differentiating under the integral sign:
  </p>
  <p>
    $$\frac{d}{d\theta} \mathbb{E}_\theta\left[\frac{\partial}{\partial\theta} \ell(\theta; X)\right] = 0 = \mathbb{E}_\theta\left[ \frac{\partial^2}{\partial\theta^2} \ell(\theta; X) + \left( \frac{\partial}{\partial\theta} \ell(\theta; X) \right)^2 \right]$$
  </p>
  <p>
    Rearranging gives the desired identity.
  </p>

  <h4>Interpretation</h4>
  <p>
    The identity connects:
    <ul>
      <li><strong>LHS</strong>: Variance of the score function</li>
      <li><strong>RHS</strong>: Negative expectation of curvature of the log-likelihood</li>
    </ul>
    Both quantify how much information the data provides about \(\theta\).
  </p>

  <h4>Summary</h4>
  <p>
    $$\mathcal{I}(\theta) = \operatorname{Var}_\theta\left( \frac{d}{d\theta} \log f(X|\theta) \right) = -\mathbb{E}_\theta\left[ \frac{d^2}{d\theta^2} \log f(X|\theta) \right]$$
  </p>
</section>

<section>
  <h4>Cram√©r‚ÄìRao Bound for Vector Parameters</h4>
  <p>
    Let:
    <ul>
      <li>\(\boldsymbol{\theta} \in \mathbb{R}^k\): vector parameter</li>
      <li>\(\mathbf{X} \sim f(\mathbf{x}|\boldsymbol{\theta})\)</li>
      <li>\(\mathbf{W}(\mathbf{X}) \in \mathbb{R}^m\): vector-valued statistic</li>
    </ul>
  </p>

  <h4>Matrix Form of the CRLB</h4>
  <p>
    $$
    \operatorname{Var}_{\boldsymbol{\theta}}(\mathbf{W}(\mathbf{X})) \succeq 
    \left( \frac{d}{d\boldsymbol{\theta}} \mathbb{E}_{\boldsymbol{\theta}}[\mathbf{W}(\mathbf{X})] \right)^\top 
    \cdot \mathcal{I}(\boldsymbol{\theta})^{-1} \cdot 
    \left( \frac{d}{d\boldsymbol{\theta}} \mathbb{E}_{\boldsymbol{\theta}}[\mathbf{W}(\mathbf{X})] \right)
    $$
  </p>

  <h4>Fisher Information Matrix</h4>
  <p>
    $$
    \mathcal{I}(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{\theta}} \left[ \left( \nabla_{\boldsymbol{\theta}} \log f(\mathbf{X}|\boldsymbol{\theta}) \right) \left( \nabla_{\boldsymbol{\theta}} \log f(\mathbf{X}|\boldsymbol{\theta}) \right)^\top \right]
    $$
  </p>
  <p>
    Where:
    $$
    \nabla_{\boldsymbol{\theta}} \log f(\mathbf{X}|\boldsymbol{\theta}) =
    \begin{bmatrix}
    \frac{\partial}{\partial\theta_1} \log f(\mathbf{X}|\boldsymbol{\theta}) \\
    \vdots \\
    \frac{\partial}{\partial\theta_k} \log f(\mathbf{X}|\boldsymbol{\theta})
    \end{bmatrix}
    $$
  </p>
  <p>
    Alternatively, if regularity conditions permit:
    $$
    \mathcal{I}(\boldsymbol{\theta}) = -\mathbb{E}_{\boldsymbol{\theta}} \left[ \nabla_{\boldsymbol{\theta}}^2 \log f(\mathbf{X}|\boldsymbol{\theta}) \right]
    $$
  </p>

  <h4>Special Case: Unbiased Estimator</h4>
  <p>
    If \(\mathbb{E}_{\boldsymbol{\theta}}[\mathbf{W}(\mathbf{X})] = \boldsymbol{\theta}\), then:
    $$
    \operatorname{Var}_{\boldsymbol{\theta}}(\mathbf{W}(\mathbf{X})) \succeq \mathcal{I}(\boldsymbol{\theta})^{-1}
    $$
  </p>

  <h4>Full CRLB Expression (Substituted Form)</h4>
  <p>
    $$
    \text{Cov}_{\boldsymbol{\theta}}(\mathbf{W}(\mathbf{X})) \succeq 
    \left[ \frac{ \partial \mathbb{E}_{\boldsymbol{\theta}}[\mathbf{W}(\mathbf{X})] }{ \partial \boldsymbol{\theta} } \right]^\top
    \left[
    \mathbb{E}_{\boldsymbol{\theta}} \left[
    \left( \frac{ \partial }{ \partial \boldsymbol{\theta} } \log f(\mathbf{X}|\boldsymbol{\theta}) \right)
    \left( \frac{ \partial }{ \partial \boldsymbol{\theta} } \log f(\mathbf{X}|\boldsymbol{\theta}) \right)^\top
    \right]
    \right]^{-1}
    \left[ \frac{ \partial \mathbb{E}_{\boldsymbol{\theta}}[\mathbf{W}(\mathbf{X})] }{ \partial \boldsymbol{\theta} } \right]
    $$
  </p>
</section>




      </section>
    </main>

  </div>

  <!-- Footer -->
  <footer class="text-center text-sm py-4 text-gray-400 border-t border-gray-700 bg-black/30 backdrop-blur-md">
    ¬© 2025 Shaan | Analysis III | Tailwind ‚ú¶ MathJax ‚ú¶ GitHub Pages
  </footer>

</body>
</html>
